---
title: "TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"
collection: publications
permalink: /publication/2024-tomg
excerpt: 'Molecule Discovery, Large Language Models'
date: 2024-12-19
venue: 'arXiv preprint'
paperurl: https://arxiv.org/pdf/2412.14642
citation: 'Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, and Qing Li. (2024). TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation. arXiv preprint arXiv:2412.14642.'
---

In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5% on TOMG-Bench.

Our project homepage is [here](https://phenixace.github.io/tomgbench/)

Our codes will be available here:
[codes are available here](https://github.com/phenixace/TOMG-Bench)

You could also download our datasets via [Huggingface Link](https://huggingface.co/datasets/Duke-de-Artois/TOMG-Bench)

[Download paper here](https://arxiv.org/pdf/2412.14642)
